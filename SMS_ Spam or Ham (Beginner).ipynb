{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3cac5ebe5effbc8c22ced3bd85a3d3e827104cf6"
   },
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQUXi3mkDlIZMmaGJzZVQnEEC535eNtp3WbO5HzZMxhCcUwucLo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83e92e96c1b2693175187f170e874f80811b915b"
   },
   "source": [
    "# **SMS: Spam or Ham (Beginner)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14f89fbf45c9e39030ace6d215aa377f75437f69"
   },
   "source": [
    "For my first kernel on Natural Language Processing (NLP), I chose the SMS Spam Collection Dataset.  \n",
    "It contains  the text of 5572 SMS messages and a label, classifying the message as \"spam\" or \"ham\".\n",
    "\n",
    "In this kernel I explore some common techniques of NLP like:\n",
    "\n",
    "* **Removing Punctuation and Stopwords**\n",
    "* **Tokenizer, Bag of words**  \n",
    "* **Term frequency inverse document frequency (TFIDF)**\n",
    "\n",
    "Based on these preprocessing, I train 6 different models that classify **unknown** messages as spam or ham. \n",
    "\n",
    "* **Naive Bayes Classifier**\n",
    "* **SVM Classifier**  \n",
    "* **KNN Classifier**\n",
    "* **SGD Classifier**\n",
    "* **Gradient Boosting Classifier**\n",
    "* **XGBoost Classifier**\n",
    "\n",
    "For easier handling of the preprocessing steps (for train and test data) and the optimization of different  \n",
    "models for the same conditions, the classification is done with **Pipelines** including GridSearchCV.  \n",
    "Finally, for the model evaluation different **metrics** are examined:  \n",
    "accuracy, precision, recall, fscore, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "501639d7c1474c946d7c02b6d592e841fee03ea5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66e435d174d98055e192cdfe780c814a3be8d89d"
   },
   "source": [
    "**The Notebook follows this outline:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "252a2abb0a3ce947720af21d20922de2187151ee"
   },
   "source": [
    "**Part 0: Imports, define functions**  \n",
    "[import libraries](#Imports)  \n",
    "define [functions](#Functions) that are used often  \n",
    "\n",
    "[**Part 1: Exploratory Data Analysis**](#Part-1:-EDA)  \n",
    "**1.1 Get an overview of the dataset**  \n",
    "head, describe and value counts  \n",
    "[Distribution of the target variable](#Distribution-of-the-target-variable)  \n",
    "[Add numerical label for spam](#Add-numerical-label-for-spam)  \n",
    "**1.2 length of message**  \n",
    "[Add feature: length of message](#Add-feature:-length-of-message)  \n",
    "**1.3 WordClouds**  \n",
    "[WordCloud: Ham messages](#WordCloud:-Ham-messages)  \n",
    "[WordCloud: Spam messages](#WordCloud:-Spam-messages)  \n",
    "\n",
    "[**Part 2: Preprocessing**](#Part-2:-Preprocessing)  \n",
    "[**2.1 Remove punctuation and stopwords**](#2.1-Remove-Punctuation-and-Stopwords)   \n",
    "[**2.2 Top 30 words in ham and spam messages**](#2.2-Top-30-words-in-ham-and-spam-messages)  \n",
    "**2.3 Bag of words with CountVectorizer**  \n",
    "[The Bag of Words representation](#The-Bag-of-Words-representation)  \n",
    "[Examples for spam and ham messages](#Examples-for-spam-and-ham-messages)  \n",
    "[Applying bow_transformer on all messages](#Applying-bow_transformer-on-all-messages)  \n",
    "**2.4 Term frequency inverse document frequency (TFIDF)**  \n",
    "[From occurrences to frequencies](#From-occurrences-to-frequencies)  \n",
    "[TfidfTransformer from sklearn](#TfidfTransformer-from-sklearn)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[**Part 3: Classifiers**](#Part-3:-Classifiers)  \n",
    "[**3.1 First test for Classification**](#3.1-First-test-for-Classification) with Naive Bayes Classifier  \n",
    "[**3.2 train test split**](#3.2-train-test-split)  \n",
    "**3.3 Pipelines for Classification of unknown messages**  \n",
    "[Multinomial Naive Bayes](#3.3.1-MultinomialNB)  (simple: Preprocessing and Classification)  \n",
    "[KNN Classifier](#3.3.2-KNN)  (GridSearchCV for model parameter)   \n",
    "[Support Vector Classifier](#3.3.3-SVC)  (GridSearchCV for Preprocessing)  \n",
    "[SGD Classifier](#3.3.4-SGD)  (GridSearchCV for Preprocessing and model parameter)  \n",
    "[GradientBoostingClassifier](#3.3.5-GradientBoostingClassifier)    (GridSearchCV for Preprocessing and model parameter)  \n",
    "[XGBoost Classifier](#3.3.6-XGBoost-Classifier)    (GridSearchCV for Preprocessing and model parameter)  \n",
    "**3.4 Comparison of results**  \n",
    "[confusion_matrix](#confusion_matrix) +++ [accuracy_score](#accuracy_score)       \n",
    "[precision_score](#precision_score) +++ [recall_score](#recall_score)  \n",
    "[f1_score](#f1_score) +++  [classification_report](#classification_report)    \n",
    "[roc_auc_score](#roc_auc_score)  \n",
    "**3.5 Optimize classifiers with scoring by precision**  \n",
    "3.5.1 [GridSearchCV pipelines version 2](#3.5.1-GridSearchCV-pipelines-version-2)  \n",
    "3.5.2 [Confusion matrices for scoring by precision](#3.5.2-Confusion-matrices-for-scoring-by-precision)  \n",
    "**3.6 Optimize classifiers with scoring by recall**  \n",
    "3.6.1 [GridSearchCV pipelines version 3](#3.6.1-GridSearchCV-pipelines-version-3)  \n",
    "3.6.2 [Confusion matrices for scoring by recall](#3.6.2-Confusion-matrices-for-scoring-by-recall)  \n",
    "**3.7 Optimize classifiers with scoring by roc_auc**  \n",
    "3.7.1 [GridSearchCV pipelines version 4](#3.7.1-GridSearchCV-pipelines-version-4)    \n",
    "3.7.2 [Confusion matrices for scoring by roc auc](#3.7.2-Confusion-matrices-for-scoring-by-roc-auc)  \n",
    "\n",
    "[**Part 4: NLTK**](#Part-4:-NLTK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "823e81881f13f82f83817a252b0fa834446f4412"
   },
   "source": [
    "**TODO : **  \n",
    " \n",
    "\n",
    "**include feature text length in model**   \n",
    "\n",
    "**NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ffaf535a4250a7fbaa1c68b27fb55d9a96cd4c0c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**  \n",
    "Parts of the EDA and preprocessing are based on the Capstone Project in Jose Portilla's Udemy course.  \n",
    "I can recommend this course for beginners in Python ML.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d3b53cdae8daf6bbb12abac40690e83ff28db5b2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d31e3961805d4b1205440831bacd4796a3d5e281"
   },
   "source": [
    "# **Part 0: Imports, define functions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e07c8e4ca9941fa0b409bbc48c79f4b432c9832"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: '../input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ac5cadbb896c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: '../input'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import wordcloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d042a0fd2b1b124201c84b367e5a3c811f3c7278"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21fed8f50a1f88bf2d614a7c193606c685beae8d"
   },
   "source": [
    "print Classification Report and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d1507ee9876985f963650eba7fe84f87eefb076"
   },
   "outputs": [],
   "source": [
    "def print_validation_report(y_true, y_pred):\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    acc_sc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy : \"+ str(acc_sc))\n",
    "    \n",
    "    return acc_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72615b9c2a0f9c82cb80879df5783abc35da6d50"
   },
   "source": [
    "plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a54d45761deee27294673d1aeadebcc1c3eed146"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    #fig, ax = plt.subplots(figsize=(4,4))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n",
    "                cmap=\"Blues\", cbar=False, ax=ax)\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eeb01c9e17e719ec7fc159a92c865efda18a8d54"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "082306c6380a6bc0a2d1aa16843ab4200f011ebe"
   },
   "source": [
    "# **Part 1: EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5654c4b08e286b2833bc03d056878325041c75c9"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/spam.csv\",encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb63ad287008768565b8f80617bb763e2ac5423d"
   },
   "source": [
    "Columns 2,3,4 contain no important data and can be deleted.  \n",
    "Also, we rename column v1 as \"label\" and v2 as \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f412489f7e767443b6e656850cca153393f0cca5"
   },
   "outputs": [],
   "source": [
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36b87f12ddaaf8db2d41ffd8d78b6a92a6634ad5"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa53b10c9da3d3e7d09fb0d29ac09d3ff84c83f6"
   },
   "outputs": [],
   "source": [
    "data.groupby(\"label\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ce5027974bdd19359abdeeb32dac9c436dcd0f2"
   },
   "source": [
    "### Distribution of the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "599f248cd4b81880ad40911f0b8c72edb89f924d"
   },
   "source": [
    "The dataset contains 4825 ham and 747 spam messages.  \n",
    "For both classes, some messages appear more than once (common phrases, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a4c90bd39e5bb963d845562e66854455b9118ca"
   },
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4f8372b931251cb5984f82969771619fbac7f2b"
   },
   "outputs": [],
   "source": [
    "data.label.value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6653bc13fd51bc3b17bea2acd62996f952a88880"
   },
   "source": [
    "### Add numerical label for spam   \n",
    "Target must be numerical for ML classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f81edcf4efc48b1a93add45419bdd6d1177a7dda"
   },
   "outputs": [],
   "source": [
    "data['spam'] = data['label'].map( {'spam': 1, 'ham': 0} ).astype(int)\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "efe33ff44f565f771a2276235239db858f3ba1ee"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 length of message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5815aab27e942c1f49e9be291121fd145d405ad0"
   },
   "source": [
    "### Add feature: length of message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "676699099d15dc5ea90e7ad15ffd6f62c1033854"
   },
   "outputs": [],
   "source": [
    "data['length'] = data['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58d7dfbc59752599edeec691c73f86656c2cc4f0"
   },
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "285f6d0fa94ecd4b114a2567e2b608159ce6c025"
   },
   "outputs": [],
   "source": [
    "data.hist(column='length',by='label',bins=60,figsize=(12,4));\n",
    "plt.xlim(-40,950);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8aa167a101b039fcbd6c2194b14b9cc0acee6e23"
   },
   "source": [
    "Looks like spam messages are generally longer than ham messages:  \n",
    "Bulk of ham has length below 100, for spam it is above 100.  \n",
    "We will check if this feature is useful for the classification task in Part 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ham  = data[data['spam'] == 0].copy()\n",
    "data_spam = data[data['spam'] == 1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(data_spam_or_ham, title):\n",
    "    text = ' '.join(data_spam_or_ham['text'].astype(str).tolist())\n",
    "    stopwords = set(wordcloud.STOPWORDS)\n",
    "    \n",
    "    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='lightgrey',\n",
    "                    colormap='viridis', width=800, height=600).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10,7), frameon=True)\n",
    "    plt.imshow(fig_wordcloud)  \n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20 )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud: Ham messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(data_ham, \"Ham messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud: Spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(data_spam, \"Spam messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc9f69c3e24cd2aa2f2559825a7ad4710e361115"
   },
   "source": [
    "# **Part 2: Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da7836b09691c2ba2a96f9af3bc8662a1fa0d788"
   },
   "source": [
    "**Basic preprocessing for common NLP tasks includes converting text to lowercase and removing punctuation and stopwords.**  \n",
    "**Further steps, especially for text classification tasks, are:**  \n",
    "* Tokenization\n",
    "* Vectorization and \n",
    "* TF-IDF weighting  \n",
    "\n",
    "**Lets apply these approaches on the SMS messages.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a89e7cb53fb78632611598e27fcaadb4d00f127"
   },
   "source": [
    "## 2.1 Remove Punctuation and Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0f0e77fc5ba947aa4585c259938064d464edfbe"
   },
   "source": [
    "### Punctuation\n",
    "**We use the punctuation list from the string library:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76654fd5e10f4c8a46d9d8c9dc5238e0a860d9af"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb9c3c22e0534e2630eed529b3989eac65a302b3"
   },
   "source": [
    "### Stopwords  \n",
    "from sklearn documentation:  https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words  \n",
    "Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text,   \n",
    "and which may be removed to avoid them being construed as signal for prediction.  \n",
    "Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.  \n",
    "\n",
    "Due to the known issues in the ’english’ stop word list of sklearn, we use the stopwords from NLTK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7fde6b90bb4cabe8eba93d07c95a91967062060"
   },
   "source": [
    "**NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b71df4ef3915bc73648a5380b29019a461305e7"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1815902b628a2ea8c7a65637767a5ca5ee161b1b"
   },
   "source": [
    "**With the above lists for punctuation characters and stop words, we define a function to remove these from the text**  \n",
    "**This function also converts all text to lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29f3ba753b6fcd26816110fe86a6995f715ad178"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation_and_stopwords(sms):\n",
    "    \n",
    "    sms_no_punctuation = [ch for ch in sms if ch not in string.punctuation]\n",
    "    sms_no_punctuation = \"\".join(sms_no_punctuation).split()\n",
    "    \n",
    "    sms_no_punctuation_no_stopwords = \\\n",
    "        [word.lower() for word in sms_no_punctuation if word.lower() not in stopwords.words(\"english\")]\n",
    "        \n",
    "    return sms_no_punctuation_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe8cec520d8684c1cd481ea26505999576df8339"
   },
   "outputs": [],
   "source": [
    "data['text'].apply(remove_punctuation_and_stopwords).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Top 30 words in ham and spam messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Collections: Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ham.loc[:, 'text'] = data_ham['text'].apply(remove_punctuation_and_stopwords)\n",
    "words_data_ham = data_ham['text'].tolist()\n",
    "data_spam.loc[:, 'text'] = data_spam['text'].apply(remove_punctuation_and_stopwords)\n",
    "words_data_spam = data_spam['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ham_words = []\n",
    "for sublist in words_data_ham:\n",
    "    for item in sublist:\n",
    "        list_ham_words.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_spam_words = []\n",
    "for sublist in words_data_spam:\n",
    "    for item in sublist:\n",
    "        list_spam_words.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ham  = Counter(list_ham_words)\n",
    "c_spam = Counter(list_spam_words)\n",
    "df_hamwords_top30  = pd.DataFrame(c_ham.most_common(30),  columns=['word', 'count'])\n",
    "df_spamwords_top30 = pd.DataFrame(c_spam.most_common(30), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.barplot(x='word', y='count', \n",
    "            data=df_hamwords_top30, ax=ax)\n",
    "plt.title(\"Top 30 Ham words\")\n",
    "plt.xticks(rotation='vertical');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.barplot(x='word', y='count', \n",
    "            data=df_spamwords_top30, ax=ax)\n",
    "plt.title(\"Top 30 Spam words\")\n",
    "plt.xticks(rotation='vertical');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 NLTK: FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_ham  = nltk.FreqDist(list_ham_words)\n",
    "fdist_spam = nltk.FreqDist(list_spam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hamwords_top30_nltk  = pd.DataFrame(fdist_ham.most_common(30),  columns=['word', 'count'])\n",
    "df_spamwords_top30_nltk = pd.DataFrame(fdist_spam.most_common(30), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.barplot(x='word', y='count', \n",
    "            data=df_hamwords_top30_nltk, ax=ax)\n",
    "plt.title(\"Top 30 Ham words\")\n",
    "plt.xticks(rotation='vertical');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.barplot(x='word', y='count', \n",
    "            data=df_spamwords_top30_nltk, ax=ax)\n",
    "plt.title(\"Top 30 Spam words\")\n",
    "plt.xticks(rotation='vertical');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22567bbef309b303857f55083a088c8f38069a50"
   },
   "source": [
    "## 2.2 Bag of words with CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4f59da6ef1965d82e67a5bcc4784382d393f9f0"
   },
   "source": [
    "### The Bag of Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d3879613c6ea148a9e7b832e9e84e832bf37073"
   },
   "source": [
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n",
    "\n",
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.  \n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "\n",
    "**Tokenization**  \n",
    "tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.  \n",
    "**Vectorization**  \n",
    "counting the occurrences of tokens in each document.  \n",
    "**TF-IDF**  \n",
    "normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.  \n",
    "\n",
    "\n",
    "**Bag of Words**  \n",
    "In this scheme, features and samples are defined as follows:\n",
    "each individual token occurrence frequency (normalized or not) is treated as a feature.  \n",
    "the vector of all the token frequencies for a given document is considered a multivariate sample.  \n",
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.  \n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors.   \n",
    "This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or “Bag of n-grams” representation.  \n",
    "Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c2d71c7f0ce91b0c88c8def0d19f514a9f73fc0"
   },
   "source": [
    "For futher details and example implementations see:  \n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model  \n",
    "https://en.wikipedia.org/wiki/Document-term_matrix  \n",
    "\n",
    "An Introduction to Bag-of-Words in NLP  \n",
    "https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3ff0bbb14d8550b32f36fc04bddc8cf081d9087"
   },
   "source": [
    "In this kernel we apply the CountVectorizer from sklearn as BOW model.  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html    \n",
    "As tokenizer we use the remove_punctuation_and_stopwords function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eda2cf09b93bc33d893d4133515ddef9f3dbf6d3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer = remove_punctuation_and_stopwords).fit(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92ca036b1a95efd0c8fbd336b747a4868a960fcb"
   },
   "outputs": [],
   "source": [
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "069f3f06649b4851e2188aade8bf16611c2ff98b"
   },
   "source": [
    "In all sms messages bow_transformer counted 9431 different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3a4998219d1f29059a895c04c18c888f715ad17"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e4f6a7394403f076dd6eef9a8927e47746370a1"
   },
   "source": [
    "### Examples for spam and ham messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf3023555a79d34bf050cbbb428198eb91c7119f"
   },
   "source": [
    "Lets look at some vectorization examples for spam and ham messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aed6b2ad047f291b51847d3dbe9d7fb48edea4fd"
   },
   "outputs": [],
   "source": [
    "sample_spam = data['text'][8]\n",
    "bow_sample_spam = bow_transformer.transform([sample_spam])\n",
    "print(sample_spam)\n",
    "print(bow_sample_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = bow_sample_spam.nonzero()\n",
    "for col in cols: \n",
    "    print(bow_transformer.get_feature_names()[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8780104e357122de05157333eff275d46526c62"
   },
   "outputs": [],
   "source": [
    "print(np.shape(bow_sample_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f6d3995c2a0b364fed178d399ae36a0cf6133d2"
   },
   "outputs": [],
   "source": [
    "sample_ham = data['text'][4]\n",
    "bow_sample_ham = bow_transformer.transform([sample_ham])\n",
    "print(sample_ham)\n",
    "print(bow_sample_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = bow_sample_ham.nonzero()\n",
    "for col in cols: \n",
    "    print(bow_transformer.get_feature_names()[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f78df8555292ca47c4000ea29214044a8a6f193e"
   },
   "source": [
    "### Applying bow_transformer on all messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7afebe912b171c75db39ff6dd2b2f326bb2ab33e"
   },
   "outputs": [],
   "source": [
    "bow_data = bow_transformer.transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c346737685259d8f2b70a9e5f49b77bbd81892a9"
   },
   "outputs": [],
   "source": [
    "bow_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e1a8ddbe80a38a8e3b233c769657f4adbc95dcce"
   },
   "outputs": [],
   "source": [
    "bow_data.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5f54d816940a9447989ad3add6b2849953a9a3e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c050910c4505978b6abf83e638b9efb6b46fc959"
   },
   "source": [
    "**Sparsity: percentage of none zero entries**  \n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n",
    "**Sparsity**  \n",
    "As most documents will typically use a very small subset of the words used in the corpus,  \n",
    "the resulting matrix will have many feature values that are zeros (typically more than 99% of them).  \n",
    "For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary  \n",
    "with a size in the order of 100,000 unique words in total while each document will use 100 to   \n",
    "1000 unique words individually.  \n",
    "In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector,  \n",
    "implementations will typically use a sparse representation such as available in the scipy.sparse package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb69e61eb5066af722d1d4389dd2a8afaaa80f5b"
   },
   "outputs": [],
   "source": [
    "bow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9386c74ce10b6c8c5b1c7d21413d3c5019e00606"
   },
   "outputs": [],
   "source": [
    "bow_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "147aecb10addaee63be626fa299555654a0449b2"
   },
   "outputs": [],
   "source": [
    "bow_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0a67e9663c17edf16718b8ecc4d40be2f15345f"
   },
   "outputs": [],
   "source": [
    "bow_data.nnz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "946ea9e4f865aba3e6d28da6b326c6e0b7b3dd0d"
   },
   "source": [
    "number of none zero entries divided by matrix size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de2aee91877b45651de69cdf576ee79a5bd6c04a"
   },
   "outputs": [],
   "source": [
    "print( bow_data.nnz / (bow_data.shape[0] * bow_data.shape[1]) *100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c58e4914d2bfc63fe69c94e1e02a8db14378cb6b"
   },
   "source": [
    "Around 10% of the matrix are non zeros (=ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e6af25790a2dc1d236703d4692eaef1a6e4ccfb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34171014a4d1bbbb89f635fa0f28561140162eef"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8968ae791a11a649e589b95236f8515d12d0dba1"
   },
   "source": [
    "## 2.3 Term frequency inverse document frequency - TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cecafb740fb49406220b1c354ce9a429e822e1e7"
   },
   "source": [
    "### From occurrences to frequencies  \n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#from-occurrences-to-frequencies\n",
    "\n",
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values  \n",
    "than shorter documents, even though they might talk about the same topics.  \n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document  \n",
    "by the total number of words in the document: these new features are called **tf for Term Frequencies**.  \n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are  \n",
    "therefore less informative than those that occur only in a smaller portion of the corpus.  \n",
    "This downscaling is called **tf–idf for “Term Frequency times Inverse Document Frequency”**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9d280f9fcc8a25e59e1dca52d5de26dc3ba3de1"
   },
   "source": [
    "For futher details and example implementations see:  \n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3f25088775f0caa568fef7164a0475d27f5302a"
   },
   "source": [
    "https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "574b03a52794e4124d47e653f44320e29f73a169"
   },
   "source": [
    "### TfidfTransformer from sklearn\n",
    "Both tf and tf–idf can be computed as follows using TfidfTransformer:   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc362886dcc6db5f68e03031d87dece50b0b500f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba16200dd77c59a8e50f1270920c50c1c386a01d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f80ca5b3bbdf2a5f89e5046fd5c7097c5a9c1d8f"
   },
   "outputs": [],
   "source": [
    "tfidf_sample_ham = tfidf_transformer.transform(bow_sample_ham)\n",
    "print(tfidf_sample_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c50ea5d0fad1e579741b3d879d4201ad059d3c1a"
   },
   "outputs": [],
   "source": [
    "tfidf_sample_spam = tfidf_transformer.transform(bow_sample_spam)\n",
    "print(tfidf_sample_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb6c826546bba11a45d88a6e65bb18a231ba0eb1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71d9302f4d0895922b5d7197338bf482410279e2"
   },
   "outputs": [],
   "source": [
    "data_tfidf = tfidf_transformer.transform(bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46472347f3522d5669bf6dd48fd759570b079378"
   },
   "outputs": [],
   "source": [
    "data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for TFIDF matrix only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_tfidf_train, data_tfidf_test, label_train, label_test = \\\n",
    "    train_test_split(data_tfidf, data[\"spam\"], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c060c8eb5dcc435b48c5b767a89a17c962893295"
   },
   "outputs": [],
   "source": [
    "data_tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for TFIDF matrix and feature \"length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import  hstack\n",
    "X2 = hstack((data_tfidf ,np.array(data['length'])[:,None])).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = \\\n",
    "    train_test_split(X2, data[\"spam\"], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f206a0ea10a5997e7509bccd25e2d72e045f56ae"
   },
   "source": [
    "# Part 3: Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "347dcda838abc7c4b452cee43076d430606a52e4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c4181849789efae863fee973649f7827f2bd20e"
   },
   "source": [
    "## 3.1 First test for Classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse matrix to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfidf_train = data_tfidf_train.A\n",
    "data_tfidf_test = data_tfidf_test.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB Model using only TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model = MultinomialNB().fit(data_tfidf_train, label_train)\n",
    "pred_test_MNB = spam_detect_model.predict(data_tfidf_test)\n",
    "acc_MNB = accuracy_score(label_test, pred_test_MNB)\n",
    "print(acc_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6cbc3816802cc4cfe056dd7d6c906955b84526fd"
   },
   "source": [
    "Our first classifier seems to work well, it has an accuracy of 96.5 % for the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_tfidf_train_sc = scaler.fit_transform(data_tfidf_train)\n",
    "data_tfidf_test_sc  = scaler.transform(data_tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB Model using only TFIDF matrix, scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model_minmax = MultinomialNB().fit(data_tfidf_train_sc, label_train)\n",
    "pred_test_MNB = spam_detect_model_minmax.predict(data_tfidf_test_sc)\n",
    "acc_MNB = accuracy_score(label_test, pred_test_MNB)\n",
    "print(acc_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the min max scaler on the TFIDF matrix improves the performance of the MNB classifier:  \n",
    "It now has an accuracy of 98.2 % for the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB model with TFIDF matrix and feature \"length\", unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model_2 = MultinomialNB().fit(X2_train, y2_train)\n",
    "pred_test_MNB_2 = spam_detect_model_2.predict(X2_test)\n",
    "acc_MNB_2 = accuracy_score(y2_test, pred_test_MNB_2)\n",
    "print(acc_MNB_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting MNB with the unscaled features TFIDF + length of message decreases performance.  \n",
    "Lets now check the fit with the scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_tfidf_train = X2_train[:,0:9431]\n",
    "X2_tfidf_test  = X2_test[:,0:9431]\n",
    "X2_length_train = X2_train[:,9431]\n",
    "X2_length_test  = X2_test[:,9431]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X2_tfidf_train = scaler.fit_transform(X2_tfidf_train)\n",
    "X2_tfidf_test  = scaler.transform(X2_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X2_length_train = scaler.fit_transform(X2_length_train.reshape(-1, 1))\n",
    "X2_length_test  = scaler.transform(X2_length_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = np.hstack((X2_tfidf_train, X2_length_train))\n",
    "X2_test  = np.hstack((X2_tfidf_test,  X2_length_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB model with TFIDF matrix and feature \"length\", scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model_3 = MultinomialNB().fit(X2_train, y2_train)\n",
    "pred_test_MNB_3 = spam_detect_model_3.predict(X2_test)\n",
    "acc_MNB_3 = accuracy_score(y2_test, pred_test_MNB_3)\n",
    "print(acc_MNB_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We studied the same classifier, Multinomial Naive Bayes, with different set of features and found that the results vary regarding the accuracy of the predictions.  \n",
    "In the following we study a different classifier, again with different set of features.\n",
    "Also we study what this accuracy actually means and also if this metric is the optimal one we should apply for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_KNN = {'n_neighbors': (10,15,17), }\n",
    "\n",
    "grid_KNN = GridSearchCV( KNeighborsClassifier(), parameters_KNN, cv=5,\n",
    "                        n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_KNN.fit(data_tfidf_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01480b009d97fd3167f36e02123f8db5abf19bb4"
   },
   "outputs": [],
   "source": [
    "print(grid_KNN.best_params_)\n",
    "print(grid_KNN.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_KNN = {'n_neighbors': (6,8,10), }\n",
    "grid_KNN = GridSearchCV( KNeighborsClassifier(), parameters_KNN, cv=5,\n",
    "                        n_jobs=-1, verbose=1)\n",
    "grid_KNN.fit(data_tfidf_train_sc, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_KNN.best_params_)\n",
    "print(grid_KNN.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1627a592628703b79643715903239d9bac9e8321"
   },
   "source": [
    "## 3.2 train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44e528fc9bbd846cbd235a7d9f6425d88fb5114e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sms_train, sms_test, label_train, label_test = \\\n",
    "    train_test_split(data[\"text\"], data[\"spam\"], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77e34b5a7b77cf0a0c1a0e111f6fd3df4a7d6410"
   },
   "outputs": [],
   "source": [
    "sms_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10423816907b50e971d57f2e5ca231f810d4c4ac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0f7c4b43b754d92c49907e2460f4316181469b6"
   },
   "source": [
    "## 3.3 Classification Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9de6be8ac49c14d38d68a16b01a0d987de546c3"
   },
   "source": [
    "After splitting the data into a train and test set we now use a pipeline to apply the   \n",
    "**CountVectorizer** and the **TfidfTransformer** on both sets.  \n",
    "We also add a classifier to the pipeline, so we can combine all necessary steps in one object:  \n",
    "* Preprecocessing  \n",
    "* Crossvalidation (GridsearchCV)\n",
    "* Fitting  \n",
    "* Predicting\n",
    "* Evaluating (test score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a46deb491be34f6996ba25d7413b81fd22d7359"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96adf620c8154e6c879193e03c6682e90a51a15c"
   },
   "source": [
    "### 3.3.1 MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**simple Pipeline. no optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73c6e99b7980f41cfd7d44f0e999eb15d4561f23"
   },
   "outputs": [],
   "source": [
    "pipe_MNB = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n",
    "                   ('tfidf'   , TfidfTransformer()),\n",
    "                   ('clf_MNB' , MultinomialNB()),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35e991c44093fe5f5bb72f65398c36f28c06402b"
   },
   "outputs": [],
   "source": [
    "pipe_MNB.fit(X=sms_train, y=label_train)\n",
    "pred_test_MNB = pipe_MNB.predict(sms_test)\n",
    "acc_MNB = accuracy_score(label_test, pred_test_MNB)\n",
    "print(acc_MNB)\n",
    "print(pipe_MNB.score(sms_test, label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two steps  \n",
    "**CountVectorizer** and **TfidfTransformer**  \n",
    "can also be performed in one step with  \n",
    "**TfidfVectorizer**  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html  \n",
    "Convert a collection of raw documents to a matrix of TF-IDF features  \n",
    "Equivalent to CountVectorizer followed by TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_MNB_tfidfvec = Pipeline([ ('tfidf_vec' , TfidfVectorizer(analyzer = remove_punctuation_and_stopwords)),\n",
    "                               ('clf_MNB'   , MultinomialNB()),\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_MNB_tfidfvec.fit(X=sms_train, y=label_train)\n",
    "pred_test_MNB_tfidfvec = pipe_MNB_tfidfvec.predict(sms_test)\n",
    "acc_MNB_tfidfvec = accuracy_score(label_test, pred_test_MNB_tfidfvec)\n",
    "print(acc_MNB_tfidfvec)\n",
    "print(pipe_MNB_tfidfvec.score(sms_test, label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yes, results are identical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 KNN  \n",
    "Pipeline with GridSearchCV  \n",
    "optimize best model parameter: n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_KNN = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n",
    "                   ('tfidf'   , TfidfTransformer()),\n",
    "                   ('clf_KNN' , KNeighborsClassifier() )\n",
    "                    ])\n",
    "\n",
    "parameters_KNN = {'clf_KNN__n_neighbors': (8,15,20), }\n",
    "\n",
    "grid_KNN = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n",
    "                        n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_KNN.fit(X=sms_train, y=label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**best_params_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNN.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cross validation score: best_score_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNN.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_grid_KNN = grid_KNN.predict(sms_test)\n",
    "acc_KNN = accuracy_score(label_test, pred_test_grid_KNN)\n",
    "print(acc_KNN)\n",
    "print(grid_KNN.score(sms_test, label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa3ebc8288ee6bfe9c391e4db9a20802e5fc1f8e"
   },
   "source": [
    "### 3.3.3 SVC  \n",
    "Pipeline with GridSearchCV  \n",
    "search best preprocessing: apply TfidfTransformer (yes/no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9528ec0130d4354a93f75459f0d223ee05a5a99"
   },
   "outputs": [],
   "source": [
    "pipe_SVC = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n",
    "                   ('tfidf'   , TfidfTransformer()),\n",
    "                   ('clf_SVC' , SVC(gamma='auto', C=1000)),\n",
    "                    ])\n",
    "\n",
    "\n",
    "parameters_SVC = dict(tfidf=[None, TfidfTransformer()],\n",
    "                      clf_SVC__C=[500, 1000,1500]\n",
    "                      )\n",
    "#parameters = {'tfidf__use_idf': (True, False),    }\n",
    "\n",
    "grid_SVC = GridSearchCV(pipe_SVC, parameters_SVC, \n",
    "                        cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_SVC.fit(X=sms_train, y=label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**best_params_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SVC.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cross validation score: best_score_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SVC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_grid_SVC = grid_SVC.predict(sms_test)\n",
    "acc_SVC = accuracy_score(label_test, pred_test_grid_SVC)\n",
    "print(acc_SVC)\n",
    "print(grid_SVC.score(sms_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 SGD  \n",
    "Pipeline with GridSearch  \n",
    "search best preprocessing: use_idf (yes/no)  \n",
    "and best model parameters (alpha, penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_SGD = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n",
    "                   ('tfidf'   , TfidfTransformer()),\n",
    "                   ('clf_SGD' , SGDClassifier(random_state=5)),\n",
    "                    ])\n",
    "\n",
    "parameters_SGD = {\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf_SGD__max_iter': (5,10),\n",
    "    'clf_SGD__alpha': (1e-05, 1e-04),\n",
    "}\n",
    "\n",
    "grid_SGD = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n",
    "                               n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_SGD.fit(X=sms_train, y=label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**best_params_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SGD.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cross validation score: best_score_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SGD.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_grid_SGD = grid_SGD.predict(sms_test)\n",
    "acc_SGD = accuracy_score(label_test, pred_test_grid_SGD)\n",
    "print(acc_SGD)\n",
    "print(grid_SGD.score(sms_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_GBC = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n",
    "                      ('tfidf'   , TfidfTransformer() ),\n",
    "                      ('clf_GBC' , GradientBoostingClassifier(random_state=5) ),\n",
    "                    ])\n",
    "\n",
    "parameters_GBC = { 'tfidf__use_idf': (True, False), \n",
    "                   'clf_GBC__learning_rate': (0.1, 0.2),\n",
    "                   #'clf_GBC__min_samples_split': (3,5), \n",
    "                 }\n",
    "\n",
    "grid_GBC = GridSearchCV(pipe_GBC, parameters_GBC, \n",
    "                        cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_GBC.fit(X=sms_train, y=label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_GBC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_GBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_grid_GBC = grid_GBC.predict(sms_test)\n",
    "acc_GBC = accuracy_score(label_test, pred_test_grid_GBC)\n",
    "print(acc_GBC)\n",
    "print(grid_GBC.score(sms_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Set params['eval_metric'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_XGB = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n",
    "                      ('tfidf'   , TfidfTransformer() ),\n",
    "                      ('clf_XGB' , xgb.XGBClassifier(random_state=5) ),\n",
    "                    ])\n",
    "\n",
    "parameters_XGB = { 'tfidf__use_idf': (True, False), \n",
    "                   'clf_XGB__eta': (0.01, 0.02),\n",
    "                   'clf_XGB__max_depth': (5,6), \n",
    "                 }\n",
    "\n",
    "grid_XGB = GridSearchCV(pipe_XGB, parameters_XGB, \n",
    "                        cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_XGB.fit(X=sms_train, y=label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_XGB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_XGB.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_grid_XGB = grid_XGB.predict(sms_test)\n",
    "acc_XGB = accuracy_score(label_test, pred_test_grid_XGB)\n",
    "print(acc_XGB)\n",
    "print(grid_XGB.score(sms_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff0832c9bdd34f6ee98c672c80f94d45877fd63c"
   },
   "source": [
    "## 3.4 Comparison of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/understanding-data-science-classification-metrics-in-scikit-learn-in-python-3bc336865019  \n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary classification task, there are 4 possible results:\n",
    "\n",
    "\n",
    "TN: True negatives  (ham mails labeled as ham)  \n",
    "FP: False positives (ham mails labeled as spam)  \n",
    "FN: False negatives (spam mails labeled as ham)  \n",
    "TP: True positives  (spam mails labeled as spam)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix gives an overview of the classification results:  \n",
    "The diagonal elements represent the number of points for which the predicted label is equal to the true label,  \n",
    "while off-diagonal elements are those that are mislabeled by the classifier.  \n",
    "The higher the diagonal values of the confusion matrix the better, indicating many correct predictions.  \n",
    "The rows of a confusion matrix correspond to the true (actual) classes and the columns correspond to the predicted classes.  \n",
    "So, all together the confusion matrix for a binary classifier consists of 4 values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TN FP  \n",
    "FN TP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using seaborn heat map for nice plot of confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    #fig, ax = plt.subplots(figsize=(4,4))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n",
    "                cmap=\"Blues\", square=True, cbar=False)\n",
    "    #  \n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary of predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62513e57732c87ae4a8d2ad765bdbe967a9a2ffb"
   },
   "outputs": [],
   "source": [
    "list_clf = [\"MNB\", \"KNN\", \"SVC\", \"SGD\", \"GBC\", \"XGB\"]\n",
    "\n",
    "list_pred = [pred_test_MNB, pred_test_grid_KNN, \n",
    "             pred_test_grid_SVC, pred_test_grid_SGD,\n",
    "             pred_test_grid_GBC, pred_test_grid_XGB]\n",
    "\n",
    "dict_pred = dict(zip(list_clf, list_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_confusion_matrices(y_true, dict_all_pred, str_title):\n",
    "    \n",
    "    list_classifiers = list(dict_all_pred.keys())\n",
    "    plt.figure(figsize=(10,7.5))\n",
    "    plt.suptitle(str_title, fontsize=20, fontweight='bold')\n",
    "    n=231\n",
    "\n",
    "    for clf in list_classifiers : \n",
    "        plt.subplot(n)\n",
    "        plot_confusion_matrix(y_true, dict_all_pred[clf])\n",
    "        plt.title(clf, fontweight='bold')\n",
    "        n+=1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_confusion_matrices(label_test, dict_pred, \"Pipelines v1, scoring=accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification accuracy = correct predictions / total predictions = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_acc = {}\n",
    "for clf in list_clf :\n",
    "    dict_acc[clf] = accuracy_score(label_test, dict_pred[clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in list_clf :\n",
    "    print(clf, \" \" , dict_acc[clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the accuracy_score with the confusion matrices, one finds that  \n",
    "accuracy score may not be the best parameter to choose the best classifier.  \n",
    "SGD, a model with high accuracy_score, incorrectly classifies 6 ham mails as spam,  \n",
    "which is usually not wanted for a spam classifier (important mails might get lost).  \n",
    "MNB has less accuracy than SGD, but it classifies all ham mails correctly.  \n",
    "SVC also classifies all ham mails correctly but compared to MNB it classifies   \n",
    "much more spam mails correctly.  \n",
    "Apart from accuracy there are further scoring methods to evaluate a classifier.  \n",
    "Lets look at the other classifier scores in more detail:  \n",
    "precision, recall, fscore, support, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is the ratio TP / (TP + FP) where TP is the number of true positives and TP the number of false positives.  \n",
    "The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.  \n",
    "Precision = 1 for FP = 0 and precision goes up when FP goes down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in list_clf :\n",
    "    print(clf, \" \", precision_score(label_test, dict_pred[clf]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition the precision is calculated for the negative class (label = 0, ham mails).  \n",
    "This is also the default when calling precision score without any further parameters.  \n",
    "But we can also examine the precision for the individual labels (ham,spam = 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in list_clf :\n",
    "    print(clf, \" \", precision_score(label_test, dict_pred[clf], average=None, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision for classifying ham mails is 1.0 for the MNB and SVC classifier.  \n",
    "SGD has the best precision for classifying ham mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall is the ratio TP / (TP + FN) where TP is the number of true positives and FN the number of false negatives.  \n",
    "The recall is intuitively the ability of the classifier to find all the positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in list_clf :\n",
    "    print(clf, \" \", recall_score(label_test, dict_pred[clf]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is defined regarding the positive class (label=1, spam mails).  \n",
    "Again, if we call the recall score method with the labels parameter, we get  \n",
    "the recall for ham and spam messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in list_clf :\n",
    "    print(clf, \" \", recall_score(label_test, dict_pred[clf], average=None, labels=[0,1] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall,  \n",
    "where an F-beta score reaches its best value at 1 and worst score at 0.  \n",
    "The F-beta score weights recall more than precision by a factor of beta.  \n",
    "beta == 1.0 means recall and precision are equally important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in list_clf :\n",
    "    print(clf, \" \", f1_score(label_test, dict_pred[clf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in list_clf :\n",
    "    print(clf, \" \", f1_score(label_test, dict_pred[clf], average=None, labels=[0,1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(label_test, pred_test_MNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support is the number of occurrences of each class in y_true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f0ebc9406c2e41b585ab2ce572f6cc5d0a351fc"
   },
   "outputs": [],
   "source": [
    "for clf in list_clf :\n",
    "    print(clf, \" \", precision_recall_fscore_support(label_test, dict_pred[clf], average=None, labels=[0,1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in list_clf :\n",
    "    print(clf, \" \", roc_auc_score(label_test, dict_pred[clf] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e4855f548722b8d2e45d17856870bed7f8abd76"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Optimize classifiers with scoring by precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform GridSearchCV again, using the same parameter grids and pipelines like before.  \n",
    "For all classifier pipelines, we only change the scoring method from \"accuracy\" to \"precision\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'precision'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 GridSearchCV pipelines version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision for MNB was already 1.0 so it can not be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNN_2 = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_KNN_2.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_KNN_2 = grid_KNN_2.predict(sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNN_2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SVC_2 = GridSearchCV(pipe_SVC, parameters_SVC, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_SVC_2.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_SVC_2 = grid_SVC_2.predict(sms_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SGD_2 = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_SGD_2.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_SGD_2 = grid_SGD_2.predict(sms_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_GBC_2 = GridSearchCV(pipe_GBC, parameters_GBC, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_GBC_2.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_GBC_2 = grid_GBC_2.predict(sms_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_XGB_2 = GridSearchCV(pipe_XGB, parameters_XGB, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_XGB_2.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_XGB_2 = grid_XGB_2.predict(sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Confusion matrices for scoring by precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_clf = [\"MNB\", \"KNN_2\", \"SVC_2\", \"SGD_2\", \"GBC_2\", \"XGB_2\"]\n",
    "\n",
    "list_pred = [pred_test_MNB, pred_test_grid_KNN_2, \n",
    "             pred_test_grid_SVC_2, pred_test_grid_SGD_2,\n",
    "             pred_test_grid_GBC_2, pred_test_grid_XGB_2]\n",
    "\n",
    "dict_pred_2 = dict(zip(list_clf, list_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_confusion_matrices(label_test, dict_pred_2, \"Pipelines v2, scoring=precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Optimize classifiers with scoring by recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For spam detection optimizing tbe classifiers by precision seems most reasonable.  \n",
    "But for other tasks it may be advantageous to have a classifier with maximum recall.  \n",
    "For example, in Credit Card Fraud detections, you want to find all fraud samples.  \n",
    "For all classifier pipelines, we perform GridSearchCV again, using the same parameter grids  \n",
    "and only changing the scoring method to \"recall\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'recall'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 GridSearchCV pipelines version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  \n",
    "paramgrid for MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNN_3 = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_KNN_3.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_KNN_3 = grid_KNN_3.predict(sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNN_3.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SVC_3 = GridSearchCV(pipe_SVC, parameters_SVC, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_SVC_3.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_SVC_3 = grid_SVC_3.predict(sms_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SGD_3 = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_SGD_3.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_SGD_3 = grid_SGD_3.predict(sms_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_GBC_3 = GridSearchCV(pipe_GBC, parameters_GBC, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_GBC_3.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_GBC_3 = grid_GBC_3.predict(sms_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_XGB_3 = GridSearchCV(pipe_XGB, parameters_XGB, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_XGB_3.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_XGB_3 = grid_XGB_3.predict(sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Confusion matrices for scoring by recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_clf = [\"MNB\", \"KNN_3\", \"SVC_3\", \"SGD_3\", \"GBC_3\", \"XGB_3\"]\n",
    "\n",
    "list_pred = [pred_test_MNB, pred_test_grid_KNN_3, \n",
    "             pred_test_grid_SVC_3, pred_test_grid_SGD_3,\n",
    "             pred_test_grid_GBC_3, pred_test_grid_XGB_3]\n",
    "\n",
    "dict_pred_3 = dict(zip(list_clf, list_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_confusion_matrices(label_test, dict_pred_3, \"Pipelines v3, scoring=recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Optimize classifiers with scoring by roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'roc_auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 GridSearchCV pipelines version 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNN_4 = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_KNN_4.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_KNN_4 = grid_KNN_4.predict(sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNN_4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thr = roc_curve(label_test, grid_KNN_4.predict_proba(sms_test)[:,1])\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Plot')\n",
    "auc_knn4 = auc(fpr, tpr) * 100\n",
    "plt.legend([\"AUC {0:.3f}\".format(auc_knn4)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SVC_4 = GridSearchCV(pipe_SVC, parameters_SVC, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_SVC_4.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_SVC_4 = grid_SVC_4.predict(sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SGD_4 = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_SGD_4.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_SGD_4 = grid_SGD_4.predict(sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_GBC_4 = GridSearchCV(pipe_GBC, parameters_GBC, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_GBC_4.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_GBC_4 = grid_GBC_4.predict(sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thr = roc_curve(label_test, grid_GBC_4.predict_proba(sms_test)[:,1])\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Plot')\n",
    "auc_gbc4 = auc(fpr, tpr) * 100\n",
    "plt.legend([\"AUC {0:.3f}\".format(auc_gbc4)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_XGB_4 = GridSearchCV(pipe_XGB, parameters_XGB, cv=5,\n",
    "                          scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_XGB_4.fit(X=sms_train, y=label_train)\n",
    "pred_test_grid_XGB_4 = grid_XGB_4.predict(sms_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 Confusion matrices for scoring by roc auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_clf = [\"MNB\", \"KNN_4\", \"SVC_4\", \"SGD_4\", \"GBC_4\", \"XGB_4\"]\n",
    "\n",
    "list_pred = [pred_test_MNB, pred_test_grid_KNN_4, \n",
    "             pred_test_grid_SVC_4, pred_test_grid_SGD_4,\n",
    "             pred_test_grid_GBC_4, pred_test_grid_XGB_4]\n",
    "\n",
    "dict_pred_4 = dict(zip(list_clf, list_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_confusion_matrices(label_test, dict_pred_4, \"Pipelines v4, scoring=roc auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['text'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent_tokenize(data['text'][7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize(data['text'][7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data['text'][7])\n",
    "wordsFiltered = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
